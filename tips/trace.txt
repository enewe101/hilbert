07 June 2019 -- 17:34:09
/home/ktsiol/projects/def-dprecup/ktsiol/hilbert/hilbert/runners/run_mle.py --cooccurrence /home/ktsiol/projects/def-dprecup/ktsiol/rldata_coocs/5w-dynamic-50k --out-dir /home/ktsiol/projects/def-dprecup/ktsiol/symmetric_embeddings/emb_49 --shard-factor 4 --learning-rate 0.0001 --updates 100000 --writes 1000 --seed 1 --temperature 2
solver_factory = build_mle_solver
save_embeddings_dir = /home/ktsiol/projects/def-dprecup/ktsiol/symmetric_embeddings/emb_49
num_writes = 1000
num_updates = 100000
monitor_closely = False
debug = False
cooccurrence_path = /home/ktsiol/projects/def-dprecup/ktsiol/rldata_coocs/5w-dynamic-50k
opt_str = adam
learning_rate = 0.0001
device = cuda:0
init_embeddings_path = None
seed = 1
dimensions = 300
verbose = True
temperature = 2.0
bias = False
shard_factor = 4
Loader: DenseLoader
Loss: MLELoss
Optimizer: ResettableOptimizer
Learner: DenseLearner
Dictionary: 50000 words

loss = 0.0018140447791665792
loss = 0.0015173028223216534
loss = 0.001375274732708931
loss = 0.001288039842620492
loss = 0.0012342262780293822
loss = 0.0011976247187703848
loss = 0.0011708943638950586
loss = 0.0011508791940286756
loss = 0.0011355761671438813
loss = 0.0011235220590606332
loss = 0.0011137683177366853
loss = 0.0011057406663894653
loss = 0.0010989734437316656
loss = 0.001093188882805407
loss = 0.0010882316855713725
loss = 0.0010839435271918774
loss = 0.001080215908586979
loss = 0.0010769456857815385
loss = 0.0010740791913121939
loss = 0.0010715252719819546
loss = 0.0010692469077184796
loss = 0.0010671892669051886
loss = 0.0010653387289494276
loss = 0.0010636377846822143
loss = 0.0010620913235470653
loss = 0.001060663489624858
loss = 0.0010593367042019963
loss = 0.001058120047673583
loss = 0.0010569874430075288
loss = 0.001055927132256329
loss = 0.0010549378348514438
loss = 0.0010540385264903307
loss = 0.0010531771695241332
loss = 0.0010523652890697122
loss = 0.0010516049806028605
loss = 0.001050904975272715
loss = 0.0010502280201762915
loss = 0.0010496078757569194
loss = 0.0010490057757124305
loss = 0.0010484462836757302
loss = 0.001047901576384902
loss = 0.001047399709932506
loss = 0.0010469190310686827
loss = 0.0010464648948982358
loss = 0.0010460162302479148
loss = 0.0010456040035933256
loss = 0.0010452083079144359
loss = 0.0010448333341628313
loss = 0.0010444741928949952
loss = 0.001044129952788353
loss = 0.0010438072495162487
loss = 0.0010434887371957302
loss = 0.0010431855916976929
loss = 0.0010428959503769875
loss = 0.001042634597979486
loss = 0.001042367541231215
loss = 0.0010421073529869318
loss = 0.0010418666061013937
loss = 0.0010416284203529358
loss = 0.001041403622366488
loss = 0.0010411882540211082
loss = 0.0010409740498289466
loss = 0.0010407775407657027
loss = 0.0010405873181298375
loss = 0.0010403962805867195
loss = 0.0010402165353298187
loss = 0.0010400436585769057
loss = 0.0010398661252111197
loss = 0.0010397103615105152
loss = 0.0010395543649792671
loss = 0.0010393926640972495
loss = 0.0010392464464530349
loss = 0.001039097085595131
loss = 0.0010389621602371335
loss = 0.0010388274677097797
loss = 0.0010386956855654716
loss = 0.001038566348142922
loss = 0.0010384444613009691
loss = 0.0010383157059550285
loss = 0.0010382018517702818
loss = 0.0010380896274000406
loss = 0.0010379806626588106
loss = 0.001037871465086937
loss = 0.0010377627331763506
loss = 0.0010376577265560627
loss = 0.0010375564452260733
loss = 0.0010374592384323478
loss = 0.0010373778641223907
loss = 0.0010372844990342855
loss = 0.0010371903190389276
loss = 0.0010371054522693157
loss = 0.001037017209455371
loss = 0.001036933739669621
loss = 0.001036852947436273
loss = 0.0010367752984166145
loss = 0.0010366988135501742
loss = 0.0010366183705627918
loss = 0.0010365478228777647
loss = 0.0010364755289629102
