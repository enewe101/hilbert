

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>hilbert — a simple embedding framework for deep learning &mdash; hilbert 0.0.1 documentation</title>
  

  
  
  
  

  

  
  
    

  

  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="#" class="icon icon-home"> hilbert
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <!-- Local TOC -->
              <div class="local-toc"><ul>
<li><a class="reference internal" href="#"><code class="docutils literal notranslate"><span class="pre">hilbert</span></code> — a simple embedding framework for deep learning</a><ul>
<li><a class="reference internal" href="#embedings"><code class="docutils literal notranslate"><span class="pre">Embedings</span></code></a></li>
</ul>
</li>
</ul>
</div>
            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="#">hilbert</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="#">Docs</a> &raquo;</li>
        
      <li><code class="docutils literal notranslate"><span class="pre">hilbert</span></code> — a simple embedding framework for deep learning</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/index.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="hilbert-a-simple-embedding-framework-for-deep-learning">
<h1><code class="docutils literal notranslate"><span class="pre">hilbert</span></code> — a simple embedding framework for deep learning<a class="headerlink" href="#hilbert-a-simple-embedding-framework-for-deep-learning" title="Permalink to this headline">¶</a></h1>
<div class="section" id="embedings">
<h2><code class="docutils literal notranslate"><span class="pre">Embedings</span></code><a class="headerlink" href="#embedings" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">hilbert.embedings.Embeddings</span></code> class allows you to save, load, and do
some basic manipulations of embeddings.  At its core, the <code class="docutils literal notranslate"><span class="pre">Embeddings</span></code> class
owns a set of (word-)vector and (context-)covector embeddings, which are
contained in two torch tensors (<code class="docutils literal notranslate"><span class="pre">torch.Tensor(dtype=torch.float32)</span></code>), along
with a dictionary that maps between words and vector indices.</p>
<p>If you just want the class to get out of your way, and give you access to those
underlying data, do this: <code class="docutils literal notranslate"><span class="pre">vectors,</span> <span class="pre">covectors,</span> <span class="pre">dictionary</span> <span class="pre">=</span> <span class="pre">embeddings</span></code>.</p>
<p>However, the <code class="docutils literal notranslate"><span class="pre">Embeddings</span></code> class provides a couple conveniences.  To begin,
you can get some random embeddings by doing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">hilbert</span> <span class="kn">as</span> <span class="nn">h</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="n">d</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span>
</pre></div>
</div>
<p>Embeddings can optionally associate a dictionary, which makes it easy to
access the embeddings that correspond to particular words.  Here we’re loading
a small dictionary used in testing, but normally you would use a dictionary
built from accumulating cooccurrence statistics:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">hilbert.test</span> <span class="kn">as</span> <span class="nn">t</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dictionary</span> <span class="o">=</span> <span class="n">t</span><span class="o">.</span><span class="n">get_test_dictionary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">random</span><span class="p">(</span><span class="mi">300</span><span class="p">,</span> <span class="mi">5000</span><span class="p">,</span> <span class="n">dictionary</span><span class="o">=</span><span class="n">dictionary</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span><span class="p">[</span><span class="s1">&#39;dog&#39;</span><span class="p">]</span>
<span class="go">tensor([0.4308, 0.9972, 0.0308, 0.6320, 0.6734, 0.9966, 0.7073, 0.2918...])</span>
</pre></div>
</div>
<p>Normally, you would either (1) get embeddings from an embedder, (2) read
embeddings previously stored on disk, or (3) make an instance by passing in
raw torch tensors or numpy arrays.</p>
<ol class="arabic simple">
<li>Get embeddings from an embedder:</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="c1"># suppose you have an embedder</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embedder</span>
<span class="go">&lt;class &#39;hilbert.torch_embedder.TorchHilbertEmbedder&#39;&gt;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">embedder</span><span class="o">.</span><span class="n">get_embeddings</span><span class="p">()</span>
</pre></div>
</div>
<ol class="arabic simple" start="2">
<li>Read previously stored embeddings:</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">embedings</span><span class="o">.</span><span class="n">Embeddings</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;my-saved-embeddings&#39;</span><span class="p">)</span>
</pre></div>
</div>
<ol class="arabic simple" start="3">
<li>Make an instance by passing in raw torch tensors or numpy arrays</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">torch</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dimensions</span><span class="p">,</span> <span class="n">vocab</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">5000</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">V</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">dimensions</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">W</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">vocab</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">embeddings</span> <span class="o">=</span> <span class="n">h</span><span class="o">.</span><span class="n">embeddings</span><span class="o">.</span><span class="n">Embeddings</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">W</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2018, Edward Newell.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.1',
            LANGUAGE:'None',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  <script type="text/javascript" src="_static/js/theme.js"></script>

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>